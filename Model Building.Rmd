```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Installing the H2o package (To be used for hyperparameter tuning in Random Forest)

```{r}
#if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
#if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }

# Next, we download packages that H2O depends on.
#pkgs <- c("RCurl","jsonlite")
#for (pkg in pkgs) {
#if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
#}

# Now we download, install and initialize the H2O package for R.
#install.packages("h2o", type="source", repos="http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/1/R")

# Finally, let's load H2O and start up an H2O cluster
library(h2o)
```



```{r}


library(tidyverse)
library(geosphere)
library(parallel)
library(corrgram)
library(corrplot)
library(mltools)
library(data.table)
library(boot)
library(glmnet)
#library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)       # an aggregator package for performing many machine learning models
#library(h2o)          # an extremely fast java-based platform



```



```{r}
airbnb = read.csv('Airbnb_NYC_2019.csv')
airbnb = airbnb %>% select(-c(host_name, name, last_review))
subway = read.csv('nyc-transit-subway-entrance-and-exit-data.csv')
```




```{r}
bnb_locations = tibble(listing = airbnb$id,
                       along = airbnb$longitude,
                       alat = airbnb$latitude)
bnb_locations = distinct(bnb_locations,listing, .keep_all = TRUE)
subway_locations = tibble(station = subway$Station.Name,
                   slong = subway$Station.Longitude,
                   slat = subway$Station.Latitude)
subway_locations = distinct(subway_locations, station, .keep_all = TRUE)
```

###################### Sample for testing ############################
 
```{r}
bnb_test = bnb_locations %>% sample_n(5)
sub_test = subway_locations %>% sample_n(5)
```



```{r}
bnb_test[,2:3]
sub_test[,2:3]
distmatrix_test = distm(bnb_test[,2:3], sub_test[,2:3])
distmatrix_test
colnames(distmatrix_test) <- sub_test$station
rownames(distmatrix_test) <- bnb_test$listing
distmatrix_test
```


```{r}
distmatrix = distm(bnb_locations[,2:3], subway_locations[,2:3])
colnames(distmatrix) <- subway_locations$station
rownames(distmatrix) <- bnb_locations$listing
```


# Finding the distance to the nearest subway station for each listing

```{r}
getClosestStation = function(distmatrix, x){
  station = colnames(distmatrix)[which.min(x)]
  distance_miles = min(x)/1609.34
  tibble(station = station, miles = distance_miles)
}
airbnb_wdist = apply(distmatrix, 1, function(x) getClosestStation(distmatrix, x)) %>% bind_rows()
airbnb_wdist
```



```{r}
airbnb = read.csv('Airbnb_NYC_2019.csv')
airbnb_all = cbind(airbnb, airbnb_wdist)

# Converting neighbourhood, neighbourhood_group and room_type to factor

airbnb_all[,c(5,6,9)] <- lapply(airbnb_all[,c(5,6,9)],as.factor)

glimpse(airbnb_all)


```


#### Model Building #####

## LINEAR REGRESSION ###

```{r}

# Checking for NAs in all columns

sapply(airbnb_all, function(x) sum(is.na(x)))

# Replacing NAs in reviews_per_month with 0

airbnb_all = airbnb_all %>% mutate(reviews_per_month = replace_na(reviews_per_month, 0))


# Getting rid of rows with price = 0  (Does not help i analysis)

airbnb_all =(airbnb_all[(airbnb_all$price != 0), ])

# Now we get rid of the columns which have no bearing on predicting price


hist(airbnb_model$minimum_nights)



min_nights_365 = airbnb_all %>% filter(minimum_nights <= 365)
min_nights_365 #48870 out of 48884

ggplot(min_nights_365,aes(x = minimum_nights)) + geom_histogram() + scale_x_continuous(breaks = seq(0,365, by = 40)) + ggtitle("Distribution of Minimum Nights") # most nights within 30 

dev.copy(png,'min_nights_365.png')
dev.off()

min_nights_30 = airbnb_all %>% filter(minimum_nights <= 30) #48137

# (48870-48137)/48884 = 1.5% of the data has min_nights > 30. Hence we get rid of those rows

ggplot(min_nights_30,aes(x = minimum_nights)) + geom_histogram() + scale_x_continuous(breaks = seq(0,30, by = 5)) + ggtitle("Distribution of Minimum Nights less than or equal to 30") # most nights within 30 

dev.copy(png,'min_nights.png')
dev.off()


airbnb_model = airbnb_all %>% filter(minimum_nights <= 30)
#hist(airbnb_model$minimum_nights)



```



```{r}

### CORRPLOT #####

airbnb_corplot <- airbnb_model[, sapply(airbnb_model, is.numeric)]
#airbnb_corplot <- airbnb_corplot[complete.cases(airbnb_corplot), ]
correlation_matrix <- cor(airbnb_corplot)
corrplot(correlation_matrix, method = "color")

dev.copy(png,'corrplot.png')
dev.off()

```


### Building the model - without removing outliers in price #####

## Splitting the data 

```{r}
set.seed(101)

# Now Selecting 70% of data as sample from total 'n' rows of the data  

sample <- sample.int(n = nrow(airbnb_model), size = floor(.70*nrow(airbnb_model)), replace = F)
training_data <- airbnb_model[sample, ]
test_data  <- airbnb_model[-sample, ]

```


# Model 1 (All regressors except miles)

```{r}

model_one <- lm(price ~ neighbourhood_group + latitude + longitude + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 , data = training_data) # Everything apart from miles, (one hot encoding of neighbourhood group, room_type) and neighbourhood

summary(model_one)


# Make predictions
predictions <- predict(model_one,newdata = test_data)

error <- (test_data$price) - predictions

RMSE <- sqrt(mean(error^2))

RMSE 

# 201.7488

```


## Model 2 - With miles

```{r}
set.seed(10)
model_two <- lm((price) ~ neighbourhood_group + latitude + longitude + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles , data = training_data) # Everything apart from miles, (one hot encoding of neighbourhood group, room_type) and neighbourhood

summary(model_two)


# Make predictions
predictions <- (predict(model_two,newdata = test_data))

error <- (test_data$price) - predictions

RMSE <- sqrt(mean(error^2))

RMSE

# 201.7454 not much improvement with the addition of miles


```




```{r}

ggplot(airbnb_model,aes(x = price)) + geom_histogram() + ggtitle("Distribution of Price")# highly skewed
dev.copy(png,'price.png')
dev.off()
summary(airbnb$price)


airbnb_model %>% filter(price > 500) %>% nrow() # 1012 out of 48137

price_less_than_500 = airbnb_model %>% filter(price < 500 ) 

b = ggplot(price_less_than_500,aes(x = price)) + geom_histogram() + scale_x_continuous(breaks = seq(0,500, by = 40))
b
dev.copy(png,"price_500.png")
dev.off()
# Most of the observations are within $280


quantile(airbnb_model$price,0.90) #90 percent of the prices fall within $269.

# Hence we remove all the observations with prices > 269


#airbnb_model %>% filter(price > 269) %>% nrow() # 4792 observations

#hist(airbnb_new_model$price)


```


### Model building with outliers in price removed

```{r}

airbnb_new_model = airbnb_model %>% filter(price <= 269)
ggplot(airbnb_new_model,aes(x = price)) + geom_histogram() + scale_x_continuous(breaks = seq(0,500, by = 40))
dev.copy(png,"price_no_outliers_2.png")
dev.off()
```

```{r}

hist(airbnb_new_model$price,col = "green",main = "Distribution of price with prices greater than $269 removed",xlab = "Price",border = "blue") 

dev.copy(png,'price_no_outliers.png')
dev.off()

set.seed(101)

# Now Selecting 70% of data as sample from total 'n' rows of the data
# 70-30 split

sample <- sample.int(n = nrow(airbnb_new_model), size = floor(.70*nrow(airbnb_new_model)), replace = F)
training_data_new <- airbnb_new_model[sample, ]
test_data_new  <- airbnb_new_model[-sample, ]


```


# Model 3

```{r}

model_three <- lm(price ~ neighbourhood_group + latitude + longitude + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 , data = training_data_new) # Everything apart from miles

summary(model_three)


# Make predictions
predictions <- predict(model_three,newdata = test_data_new)

error <- (test_data_new$price) - predictions

RMSE <- sqrt(mean(error^2))

RMSE 

# RMSE = 41.2377

```

### Model 4: Since the coefficients are not interpretable due to the latitude and longitude values (potentially), we get rid of those two regressors and add miles as the additional variable which takes into account the distance of the nearest subway station for each listing.

```{r}
set.seed(80)

model_four <- lm(price ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles , data = training_data_new) # Everything apart from miles

summary(model_four)


# Make predictions
predictions <- predict(model_four,newdata = test_data_new)

error <- (test_data_new$price) - predictions

RMSE <- sqrt(mean(error^2))

RMSE 

#RMSE 42.03 with no CV

model_four

```


#### Model with ethnicity

```{r,warning=FALSE}

library (readr)

urlfile="https://raw.githubusercontent.com/dlsimpao/AirbnbStudy/main/EthnicityPredictions.csv"

ethn_pred <-read_csv(url(urlfile))


airbnb_eth_model = cbind(airbnb,race = ethn_pred$race)

airbnb_eth_model =(airbnb_eth_model[(airbnb_eth_model$price != 0), ])



airbnb_eth_model = airbnb_eth_model %>% filter(minimum_nights <= 30)
#hist(airbnb_model$minimum_nights)

airbnb_eth_model = airbnb_eth_model %>% filter(price <= 269)

airbnb_eth_model = cbind(airbnb_new_model,ethnicity = airbnb_eth_model$race)

airbnb_eth_model$ethnicity = as.factor(airbnb_eth_model$ethnicity)

#dim(airbnb_eth_model)


#colnames(airbnb_eth_model)


```

#### Model 

```{r}
set.seed(55)

# Now Selecting 70% of data as sample from total 'n' rows of the data
# 70-30 split

sample <- sample.int(n = nrow(airbnb_eth_model), size = floor(.70*nrow(airbnb_eth_model)), replace = F)
training_data_eth <- airbnb_eth_model[sample, ]
test_data_eth  <- airbnb_eth_model[-sample, ]


model_eth <- lm(price ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + as.factor(ethnicity) + miles , data = training_data_eth) # Everything apart from miles

summary(model_eth)

### Etnicity not very significant ####

```


### English names vs Non-English

```{r}
dummy_ethnicity <- one_hot(as.data.table(airbnb_eth_model$ethnicity))

airbnb_reduced_model <- cbind(airbnb_eth_model,dummy_ethnicity)

airbnb_reduced_model <- airbnb_reduced_model[,c(1:19,25)]

airbnb_reduced_model$English = airbnb_reduced_model$`V1_GreaterEuropean,British`

airbnb_reduced_model <- airbnb_reduced_model[,c(1:19,21)]

del_rows = which(is.na(airbnb_reduced_model$English))

airbnb_reduced_model <- airbnb_reduced_model[-c(del_rows),]

airbnb_reduced_model <- airbnb_reduced_model[,c(1:18,20)]

```


```{r}

set.seed(101)

sample <- sample.int(n = nrow(airbnb_reduced_model), size = floor(.70*nrow(airbnb_reduced_model)), replace = F)
training_data_red <- airbnb_reduced_model[sample, ]
test_data_red  <- airbnb_reduced_model[-sample, ]


model_red_eth <- lm(price ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English , data = training_data_red) 

summary(model_red_eth)


# Make predictions
predictions <- predict(model_red_eth,newdata = test_data_red)

error <- (test_data_red$price) - predictions

RMSE <- sqrt(mean(error^2))



RMSE

### Hosts with english names, on average, charge $2.06 more than hosts with non-english names.

```


### Cross validated final model 

```{r,warning=FALSE}
set.seed(80)

train.control <- trainControl(method = "cv", number = 5)

model_red_eth_cv <- train(price ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + as.factor(English) , data = airbnb_reduced_model,trainControl = train.control,method = "lm") # Everything apart from miles

print(model_red_eth_cv) #R2 = 0.4948, MSE = 41.60

summary(model_red_eth_cv)


```




```{r}


airbnb_eth_model_logit = cbind(airbnb, ethn_pred$race)

airbnb_eth_model_logit =(airbnb_eth_model[(airbnb_eth_model_logit$price != 0), ])

airbnb_all = cbind(airbnb_all,ethn = airbnb_eth_model_logit$ethnicity)

dummy_ethn <- one_hot(as.data.table(airbnb_all$ethn))

airbnb_all = cbind(airbnb_all,dummy_ethn)

airbnb_all = airbnb_all[,c(1:18,25)]


airbnb_all$English = airbnb_all$`V1_GreaterEuropean,British`

airbnb_all <- airbnb_all[,c(1:18,20)]

del_rows = which(is.na(airbnb_all$English))

airbnb_all <- airbnb_all[-c(del_rows),]



```





######################### Logit Regression ##########################

```{r}

#logistic regression on price
#affordable, medium, expensive, very expensive
summary(airbnb_all$price)
# affordable (10, 69)
quantile(airbnb_all$price,0.25)
# medium (69, 106)
quantile(airbnb_all$price, 0.50)
# expensive (106,269)
quantile(airbnb_all$price, 0.75)
# very expensive (269, )
airbnb_all_pricelogit = airbnb_all %>%
  mutate(affordable = case_when(
    price < 69 ~ 1,
    TRUE ~ 0
  ), medium = case_when(
    (price >= 69 & price < 106) ~ 1,
    TRUE ~ 0
  ), expensive = case_when(
    (price >= 106 & price < 175) ~ 1,
    TRUE ~ 0
  ), very_expensive = case_when(
    (price >= 175) ~ 1,
    TRUE ~ 0
  )
)
airbnb_all_pricelogit %>% transmute(price, affordable, medium, expensive, very_expensive) %>% colSums()
#View(airbnb_all_pricelogit)

```



```{r}
#logistic regression using glm()
set.seed(98)
sample <- sample.int(n = nrow(airbnb_all_pricelogit), size = floor(.70*nrow(airbnb_all_pricelogit)), replace = F)
training_data_logit <- airbnb_all_pricelogit[sample, ]
test_data_logit  <- airbnb_all_pricelogit[-sample, ]
logit_affordable = glm(affordable ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English, data = training_data_logit, family = "binomial")
#summary(logit_affordable)

```


```{r}
predicted_affordable = predict.glm(logit_affordable, test_data_logit, type = "response")
predicted_affordable = ifelse(predicted_affordable > 0.5, 1, 0)
mean(predicted_affordable == test_data_logit$affordable)
table(predicted_affordable,test_data_logit$affordable)
```

# Cross Validation
```{r}
logit_affordable2 = glm(affordable ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English, data = airbnb_all_pricelogit, family = "binomial")
cv.glm(airbnb_all_pricelogit, logit_affordable2, K = 5)$delta
``` 


# Medium price

```{r}
logit_medium = glm(medium ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English, data = airbnb_all_pricelogit, family = "binomial")
cv.glm(airbnb_all_pricelogit, logit_medium, K = 5)$delta
```

```{r}
logit_medium2 = glm(medium ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English, data = training_data_logit, family = "binomial")

predicted_medium = predict.glm(logit_medium2, test_data_logit, type = "response")
predicted_medium = ifelse(predicted_medium > 0.5, 1, 0)
mean(predicted_medium == test_data_logit$medium)
table(predicted_medium,test_data_logit$medium)
``` 


# Expensive price

```{r}
logit_expensive = glm(expensive ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English, data = airbnb_all_pricelogit, family = "binomial")
cv.glm(airbnb_all_pricelogit, logit_expensive, K = 5)$delta
``` 

```{r}
logit_expensive2 = glm(expensive ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English, data = training_data_logit, family = "binomial")

predicted_expensive = predict.glm(logit_expensive2, test_data_logit, type = "response")
predicted_expensive = ifelse(predicted_expensive > 0.5, 1, 0)
mean(predicted_expensive == test_data_logit$expensive)
table(predicted_expensive,test_data_logit$expensive)
```


# Very expensive 

```{r}
logit_very_expensive = glm(very_expensive ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English, data = airbnb_all_pricelogit, family = "binomial")
cv.glm(airbnb_all_pricelogit, logit_very_expensive, K = 5)$delta
``` 

```{r}
logit_very_expensive2 = glm(very_expensive ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + availability_365 + miles + English, data = training_data_logit, family = "binomial")

predicted_very_expensive = predict.glm(logit_very_expensive2, test_data_logit, type = "response")
predicted_very_expensive = ifelse(predicted_very_expensive > 0.5, 1, 0)
mean(predicted_very_expensive == test_data_logit$very_expensive)
table(predicted_very_expensive,test_data_logit$very_expensive)
```


#### Logistic regression for availabilty ######

```{r}

#logistic regression on availability
# short-rental, medium-rental, long-rental 
summary(airbnb_all$availability_365)

airbnb_all_availabiltiylogit = airbnb_all %>%
  mutate(short_rental = case_when(
    availability_365 <= 30 ~ 1,
    TRUE ~ 0
  ), medium_rental= case_when(
    (availability_365 > 30 & availability_365 <= 180) ~ 1,
    TRUE ~ 0
  ), long_rental = case_when(
    (availability_365 > 180) ~ 1,
    TRUE ~ 0
  )
)
airbnb_all_availabiltiylogit %>% transmute(availability_365, short_rental, medium_rental, long_rental) %>% colSums()

#View(airbnb_all_availabiltiylogit)



```



```{r,warning=FALSE}
#logistic regression using glm()

set.seed(123)
sample <- sample.int(n = nrow(airbnb_all_availabiltiylogit), size = floor(.70*nrow(airbnb_all_availabiltiylogit)), replace = F)
training_data_logit <- airbnb_all_availabiltiylogit[sample, ]
test_data_logit  <- airbnb_all_availabiltiylogit[-sample, ]
logit_short_rental = glm(short_rental ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + miles + English, data = training_data_logit, family = "binomial")


```



```{r}
predicted_short_rental = predict.glm(logit_short_rental, test_data_logit, type = "response")
predicted_short_rental = ifelse(predicted_short_rental > 0.5, 1, 0)
mean(predicted_short_rental == test_data_logit$short_rental)
table(predicted_short_rental,test_data_logit$short_rental)

```

# Cross Validation
```{r,warning=FALSE}

logit_short_rental2 = glm(short_rental ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + miles + English, data = airbnb_all_availabiltiylogit, family = "binomial")
cv.glm(airbnb_all_availabiltiylogit, logit_short_rental2, K = 5)$delta

#summary(airbnb_all_availabiltiylogit$short_rental)

``` 




# Medium Rental


```{r}
#logistic regression using glm()

set.seed(123)
sample <- sample.int(n = nrow(airbnb_all_availabiltiylogit), size = floor(.70*nrow(airbnb_all_availabiltiylogit)), replace = F)
training_data_logit <- airbnb_all_availabiltiylogit[sample, ]
test_data_logit  <- airbnb_all_availabiltiylogit[-sample, ]
logit_medium_rental = glm(medium_rental ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + miles + English, data = training_data_logit, family = "binomial")



```



```{r}
predicted_medium_rental = predict.glm(logit_medium_rental, test_data_logit, type = "response")
predicted_medium_rental = ifelse(predicted_medium_rental > 0.5, 1, 0)
mean(predicted_medium_rental == test_data_logit$medium_rental)
table(predicted_medium_rental,test_data_logit$medium_rental)

```

# Cross Validation
```{r}

logit_medium_rental2 = glm(medium_rental ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + miles + English, data = airbnb_all_availabiltiylogit, family = "binomial")
cv.glm(airbnb_all_availabiltiylogit, logit_medium_rental2, K = 5)$delta

``` 


# Long rental 

```{r}
#logistic regression using glm()

set.seed(123)
sample <- sample.int(n = nrow(airbnb_all_availabiltiylogit), size = floor(.70*nrow(airbnb_all_availabiltiylogit)), replace = F)
training_data_logit <- airbnb_all_availabiltiylogit[sample, ]
test_data_logit  <- airbnb_all_availabiltiylogit[-sample, ]
logit_long_rental = glm(long_rental ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + miles + English, data = training_data_logit, family = "binomial")



```



```{r}
predicted_long_rental = predict.glm(logit_long_rental, test_data_logit, type = "response")
predicted_long_rental = ifelse(predicted_long_rental > 0.5, 1, 0)
mean(predicted_long_rental == test_data_logit$long_rental)
table(predicted_long_rental,test_data_logit$long_rental)

```

# Cross Validation
```{r,warning=FALSE}

logit_long_rental2 = glm(long_rental ~ neighbourhood_group + room_type + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count + miles + English, data = airbnb_all_availabiltiylogit, family = "binomial")
cv.glm(airbnb_all_availabiltiylogit, logit_long_rental2, K = 5)$delta

``` 



### Creating dummy variables for lasso #####

```{r}
#library(mltools)
#library(data.table)

dummy_roomtype <- one_hot(as.data.table(airbnb_reduced_model$room_type))

dummy_neighbourhoodgroup <- one_hot(as.data.table(airbnb_reduced_model$neighbourhood_group))

dummy_neighbourhood <- one_hot(as.data.table(airbnb_reduced_model$neighbourhood))


airbnb_lasso = airbnb_reduced_model %>% cbind(c(dummy_neighbourhoodgroup,dummy_roomtype,dummy_neighbourhood))

airbnb_lasso = airbnb_lasso %>% select(-c(id,name,host_id,host_name,neighbourhood_group,neighbourhood,latitude,longitude,room_type,last_review,station))

airbnb_lasso[,8:237] <- lapply(airbnb_lasso[,8:237], as.factor)


```



### Splitting data ####

```{r}

set.seed(111)

sample <- sample.int(n = nrow(airbnb_lasso), size = floor(.70*nrow(airbnb_lasso)), replace = F)
training_lasso <- airbnb_lasso[sample, ]
test_lasso  <- airbnb_lasso[-sample, ]


```



### Lasso ####

```{r}

#Response Variable

y.train <- training_lasso$price

# Set of predictor variables

x.train <- data.matrix(training_lasso %>% select(-c(price)))

### Fitting the Lasso model ##### (alpha = 1)

#perform k-fold cross-validation (with k = 10) to find optimal lambda value
cv_model <- cv.glmnet(x.train, y.train, alpha = 1) 

plot(cv_model)
dev.copy(png,"lasso.png")
dev.off()

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda #0.1528974


#find coefficients of best model
best_model <- glmnet(x.train, y.train, alpha = 1, lambda = best_lambda)
x = coef(best_model) #coefficients for lasso 



y.test <- test_lasso$price

# Set of predictor variables

x.test <- data.matrix(test_lasso %>% select(-c(price)))

y_predicted <- predict.glmnet(best_model, s = best_lambda, newx = x.test)

#head(y_predicted)



#newX <- model.matrix(y.test ~ x.test , data = test_data_new)
#fit_test<-predict(best_model, newX, s = best_lambda)

error <- (test_lasso$price) - y_predicted

RMSE <- sqrt(mean(error^2))

RMSE  ### 38.98 (lesser than linear reg model 5)

best_model$dev.ratio #0.5592575 R2. Better than simple linear regression model four


```



## Random Forest ##

```{r}
set.seed(75)
airbnb_rf = airbnb_lasso %>% as_tibble()
#tidy.name.vector <- make.names(names(airbnb_rf), unique=TRUE)
#names(airbnb_rf) <- tidy.name.vector

#airbnb_rf[,8:236] <- lapply(airbnb_rf[,8:236], as.factor)


colnames(airbnb_rf)<-gsub("V1","",colnames(airbnb_rf))
colnames(airbnb_rf)<-gsub("_","",colnames(airbnb_rf))
colnames(airbnb_rf)<-gsub(" ","",colnames(airbnb_rf))
colnames(airbnb_rf)<-gsub("/","",colnames(airbnb_rf))
colnames(airbnb_rf)<-gsub("'","",colnames(airbnb_rf))
colnames(airbnb_rf)<-gsub("-","",colnames(airbnb_rf))
colnames(airbnb_rf)<-gsub(",","",colnames(airbnb_rf))

#names(airbnb_rf) <- make.names(names(airbnb_rf))


sample <- sample.int(n = nrow(airbnb_rf), size = floor(.70*nrow(airbnb_rf)), replace = F)
training_rf <- airbnb_rf[sample, ]
test_rf  <- airbnb_rf[-sample, ]

#glimpse(airbnb_rf)

```


# Testing with dummy variables 

```{r}
rf_dummy <- ranger(
    formula   = price ~ ., 
    data      = training_rf, 
    num.trees = 500,
    mtry      = floor((ncol(training_rf)/3))
  )

rf_dummy # Rsquared = 0.589, OOB pred error 1401.582
```




### Testing without dummy var 
```{r}

set.seed(75)

airbnb_rf_no_dummy <- airbnb_new_model %>% select(-c(id,name,host_id,host_name,latitude,longitude,last_review,station))

tidy.name.vector <- make.names(names(airbnb_rf_no_dummy), unique=TRUE)
names(airbnb_rf_no_dummy) <- tidy.name.vector

sample <- sample.int(n = nrow(airbnb_rf_no_dummy), size = floor(.70*nrow(airbnb_rf_no_dummy)), replace = F)
training_rf_no_dummy <- airbnb_rf_no_dummy[sample, ]
test_rf_no_dummy  <- airbnb_rf_no_dummy[-sample, ]




rf_no_dummy <- ranger(
    formula   = price ~ ., 
    data      = training_rf_no_dummy, 
    num.trees = 500,
    mtry      = floor((ncol(training_rf_no_dummy)/3))
  )

rf_no_dummy   #R squared = 0.578, OOB prediction error (MSE) = 1440.389


```



### Grid Search for hyperparameter tuning WITH dummy variables ###


```{r}
hyper_grid_2 <- expand.grid(
  mtry       = seq(15, 200, by = 25),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.632, .70, .80),
  OOB_RMSE  = 0
)

# perform grid search
for(i in 1:nrow(hyper_grid_2)) {
  
  # train model
  model <- ranger(
    formula         = price ~ ., 
    data            = training_rf, 
    num.trees       = 500,
    mtry            = hyper_grid_2$mtry[i],
    min.node.size   = hyper_grid_2$node_size[i],
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid_2$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid_2 %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(50)


```


### Testing with parameters obtained from the grid search (mtry = 65 and node size = 5)

```{r,warning=FALSE}

optimal_model <- ranger(
  formula         = price ~ ., 
  data            = training_rf, 
  num.trees       = 500,
  mtry            = 65,
  min.node.size   = 5,
  sample.fraction = .632,
  importance      = 'impurity'
  )

optimal_model



#Ranger result

#Call:
 #ranger(formula = price ~ ., data = training_rf, num.trees = 500,      mtry = 65, min.node.size = 5, sample.fraction = #0.632, importance = "impurity") 

#Type:                             Regression 
#Number of trees:                  500 
#Sample size:                      30341 
#Number of independent variables:  235 
#Mtry:                             65 
#Target node size:                 5 
#Variable importance mode:         impurity 
#Splitrule:                        variance 
#OOB prediction error (MSE):       1390.888 (37.29)
#R squared (OOB):                  0.5928842 



new_train = training_rf %>% select(c(-price))

new_test <- test_rf %>% select(c(-price))

y_predicted <- predict(optimal_model, new_test,type = "response")

error <- (test_rf$price) - y_predicted$predictions

RMSE <- sqrt(mean(error^2))

RMSE #38.08

```


# Graphing top 25 important variables

```{r}

library(broom)

optimal_model$variable.importance %>% 
  tidy() %>%
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col() +
  coord_flip() +
  ggtitle("Top 25 important variables") + ylab("Reduction in MSE") + xlab("Predictor")

dev.copy(png,"impvar_rf.png")
dev.off()


```



### Grid Search for hyperparameter tuning WITHOUT dummy variables ###


```{r}
hyper_grid_3 <- expand.grid(
  mtry       = seq(2,8,by = 2),
  node_size  = seq(3,9, by = 2),
  sampe_size = c(.632, .70, .80),
  OOB_RMSE  = 0
)

# perform grid search
for(i in 1:nrow(hyper_grid_3)) {
  
  # train model
  model <- ranger(
    formula         = price ~ ., 
    data            = training_rf_no_dummy, 
    num.trees       = 500,
    mtry            = hyper_grid_3$mtry[i],
    min.node.size   = hyper_grid_3$node_size[i],
    sample.fraction = hyper_grid_3$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid_3$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid_3 %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(50)



```




```{r}
optimal_model_no_dummy <- ranger(
  formula         = price ~ ., 
  data            = training_rf_no_dummy, 
  num.trees       = 500,
  mtry            = 4,
  min.node.size   = 9,
  sample.fraction = .70,
  importance      = 'impurity'
  )

optimal_model_no_dummy

```




#### Using H20 package for parameter tuning (with dummy variables) #######

```{r}
h2o.init(max_mem_size = "24g")
```


```{r}
y <- "price"
x <- setdiff(names(training_rf),y)


train.h2o <- as.h2o(training_rf)



```


## Setting tuning parameters

```{r}

# hyperparameter grid
hyper_grid.h2o <- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(15,205, by = 25),
  max_depth   = seq(25, 115, by = 15),
  min_rows    = seq(1, 5, by = 2),
  nbins       = seq(10, 30, by = 5),
  sample_rate = c(.55, .632, .75)
)

# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 45*60
  )

# build grid search 
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x, 
  y = y, 
  training_frame = as.h2o(training_rf),
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )

# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid2", 
  sort_by = "mse", 
  decreasing = FALSE
  )
print(grid_perf2)


summary(grid_perf2, show_stack_traces = TRUE)

grid_perf2

```

## Taking the best model

```{r}

# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Now let’s evaluate the model performance on a test set
airbnb_test.h2o <- as.h2o(test_rf)
best_model_perf <- h2o.performance(model = best_model, newdata = airbnb_test.h2o)

# RMSE of best modelai
h2o.mse(best_model_perf) %>% sqrt()





```


# Shut down H20 connection 

```{r}
#h2o.shutdown()

```


